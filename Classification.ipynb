{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_data import TextsinlevelsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsinlevels = TextsinlevelsDB(db_name=\"textsinlevels\")\n",
    "df_news = textsinlevels.write_from_table_to_df(\"newsinlevels\")\n",
    "df_days = textsinlevels.write_from_table_to_df(\"daysinlevels\")\n",
    "del textsinlevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from wordfreq import zipf_frequency\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.lemmas = self.lemmatize()\n",
    "    \n",
    "    def lemmatize(self):\n",
    "        return [token.lemma_.lower() for token in self.tokens\n",
    "                       if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    def count_words(self):\n",
    "        n_words = 0\n",
    "        for token in self.tokens:\n",
    "            if token.is_alpha:\n",
    "                n_words += 1\n",
    "        return n_words\n",
    "    \n",
    "    def count_sentences(self):\n",
    "        n_sentences = 0\n",
    "        for sent in self.tokens.sents:\n",
    "            n_sentences += 1\n",
    "        return n_sentences\n",
    "    \n",
    "    def count_type_token_ratio(self):\n",
    "        return len(set(self.lemmas)) / len(self.lemmas) \n",
    "    \n",
    "    def count_words_from_wordlist(self, wordlist):\n",
    "        words_from_wordlists = 0\n",
    "        \n",
    "        for lemma in self.lemmas:\n",
    "            if lemma in wordlist:\n",
    "                words_from_wordlists += 1\n",
    "        \n",
    "        return words_from_wordlists / len(self.lemmas)\n",
    "    \n",
    "    def count_words_from_level_lists(self, word2level):\n",
    "        levels = (\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\")\n",
    "        level_freqs = {level: 0 for level in levels}\n",
    "        \n",
    "        for lemma in self.lemmas:\n",
    "            level = word2level.get(lemma)\n",
    "            if level:\n",
    "                level_freqs[level] += 1\n",
    "\n",
    "        for level in level_freqs:\n",
    "            level_freqs[level] /= len(self.lemmas)\n",
    "            \n",
    "        return level_freqs\n",
    "    \n",
    "    def count_zipf_freqs(self):\n",
    "        zipf_freqs = {}\n",
    "        \n",
    "        for lemma in self.lemmas:\n",
    "            zipf_freq = math.floor(zipf_frequency(lemma, \"en\"))\n",
    "            if zipf_freq in zipf_freqs:\n",
    "                zipf_freqs[zipf_freq] += 1\n",
    "            else:\n",
    "                zipf_freqs[zipf_freq] = 1\n",
    "        \n",
    "        for zipf_freq in zipf_freqs:\n",
    "            zipf_freqs[zipf_freq] /= len(self.lemmas)\n",
    "        \n",
    "        return zipf_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, dataset, dataset_name, nlp, preprocess):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = dataset\n",
    "        if preprocess:\n",
    "            docs = spacy.tokens.DocBin(store_user_data=False)\n",
    "            for doc in nlp.pipe(tqdm(self.dataset[\"article_text\"])):\n",
    "                docs.add(doc)\n",
    "            docs.to_disk(self.dataset_name)\n",
    "        else:\n",
    "            docs = spacy.tokens.DocBin().from_disk(self.dataset_name)\n",
    "        self.texts = [Text(doc) for doc in docs.get_docs(spacy.blank(\"en\").vocab)]\n",
    "    \n",
    "    def count_words(self):\n",
    "        return [text.count_words() for text in self.texts]\n",
    "    \n",
    "    def count_sentences(self):\n",
    "        return [text.count_sentences() for text in self.texts]\n",
    "    \n",
    "    def lemmatize(self):\n",
    "        return [text.lemmatize() for text in self.texts]\n",
    "    \n",
    "    def count_type_token_ratio(self):\n",
    "        return [text.count_type_token_ratio() for text in self.texts]\n",
    "    \n",
    "    def count_words_from_wordlist(self, wordlist):\n",
    "        return [text.count_words_from_wordlist(wordlist) for text in self.texts]\n",
    "    \n",
    "    def count_words_from_level_lists(self, word2level):\n",
    "        return [text.count_words_from_level_lists(word2level) for text in self.texts]\n",
    "        \n",
    "    def count_zipf_freqs(self):\n",
    "        return [text.count_zipf_freqs() for text in self.texts]\n",
    "                \n",
    "    def show_counts_info(self):\n",
    "        print(self.dataset_name)\n",
    "        d = {\"Number of words\": self.count_words(),\n",
    "            \"Number of sentences\": self.count_sentences()}\n",
    "        df_stats = pd.DataFrame(d)\n",
    "        print(df_stats.describe())\n",
    "\n",
    "        f = plt.figure(figsize=(10, 4))\n",
    "        plt.suptitle(self.dataset_name)\n",
    "        gs = f.add_gridspec(1, 2)\n",
    "\n",
    "        for i, col in enumerate(d):\n",
    "            ax = f.add_subplot(gs[0, i])\n",
    "            ax = sns.distplot(df_stats[col], bins=20)\n",
    "\n",
    "        plt.show()\n",
    "        f.savefig(f\"{self.dataset_name}-words_sentences_counts.png\")\n",
    "    \n",
    "    def create_lexical_df(self, abstract_nouns, concrete_nouns):\n",
    "        zipf_freqs = self.count_zipf_freqs()\n",
    "        df_lexical = pd.DataFrame({f\"zipf_freqs_{i}\": [dct.get(i, 0) for dct in zipf_freqs]\n",
    "                                     for i in range(1, 7)})\n",
    "        for level in levels:\n",
    "            df_lexical[level] = [dct[level] for dct in self.count_words_from_level_lists(word2level)]\n",
    "\n",
    "        df_lexical[\"type_token_ratio\"] = self.count_type_token_ratio()\n",
    "        df_lexical[\"abstract_nouns\"] = self.count_words_from_wordlist(abstract_nouns)\n",
    "        df_lexical[\"concrete_nouns\"] = self.count_words_from_wordlist(concrete_nouns)\n",
    "        df_lexical[\"level\"] = self.dataset[\"level\"]\n",
    "        return df_lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_lg\"\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = TextDataset(dataset=df_news, dataset_name=\"news\", nlp=nlp, preprocess=False)\n",
    "days = TextDataset(dataset=df_days, dataset_name=\"days\", nlp=nlp, preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lemmas = [\" \".join(news.texts[i].lemmas) for i in range(len(news.texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTfidfVectorizer(TfidfVectorizer):\n",
    "        \n",
    "    def fit_transform(self, raw_documents, y):\n",
    "        X = TfidfVectorizer.fit_transform(self, raw_documents, y=None)\n",
    "        return X/X.sum(axis=1)\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        X = TfidfVectorizer.transform(self, raw_documents)\n",
    "        X.sum(axis=1)\n",
    "        return X/X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = list(news.dataset[\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word_list(filename):\n",
    "    with open(f\"wordlists/{filename}\", encoding='utf-8') as f:\n",
    "        words_from_list = f.read().split('\\n')\n",
    "        \n",
    "    words_from_list = ' '.join([w for w in words_from_list if \" \" not in w and \"-\" not in w])\n",
    "    words_from_list = nlp(words_from_list)\n",
    "    words_from_list = set(w.lemma_.lower() for w in words_from_list)\n",
    "    return words_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 384\n"
     ]
    }
   ],
   "source": [
    "abstract_nouns = preprocess_word_list(\"abstract_nouns.txt\")\n",
    "concrete_nouns = preprocess_word_list(\"concrete_nouns.txt\")\n",
    "print(len(abstract_nouns), len(concrete_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}\n",
    "df = pd.read_csv(\"wordlists/Vocabulary Framework – British English.csv\")\n",
    "sub_df = df[[\"Base Word\", \"Level\"]]\n",
    "sub_df_min = sub_df.groupby(\"Base Word\").min()\n",
    "for d, data in sub_df_min.reset_index().groupby(\"Level\"):\n",
    "    dct[d] = list(data[\"Base Word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2level = {}\n",
    "for level, words in dct.items():\n",
    "    for i in range(len(words)):\n",
    "        if ' ' not in words[i]:\n",
    "            word2level[words[i].lower()] = level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = (\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lexical_df = news.create_lexical_df(abstract_nouns, concrete_nouns)\n",
    "days_lexical_df = days.create_lexical_df(abstract_nouns, concrete_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lexical_df[\"lemmas\"] = news_lemmas\n",
    "news_lexical_df.drop(columns=[\"level\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(news_lemmas, target, \n",
    "#                                                     train_size=0.8, \n",
    "#                                                     random_state=42,\n",
    "#                                                    stratify=target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(news_lexical_df, target, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42,\n",
    "                                                   stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "vectorizer = MyTfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\"naive_Bayes\": ComplementNB(),\n",
    "        \"lr\": LogisticRegression(),\n",
    "        \"svm\": LinearSVC(),\n",
    "        \"tree\": DecisionTreeClassifier(),\n",
    "        \"adaboost\": AdaBoostClassifier(),\n",
    "        \"random_forest\": RandomForestClassifier(),\n",
    "        \"extra_trees\": ExtraTreesClassifier(),\n",
    "        \"lgbm\": LGBMClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_based = {\"tree\", \"adaboost\", \"random_forest\", \"extra_trees\", \"lgbm\"}\n",
    "with_coefs = {\"lr\", \"svm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[(\"vectorizer\", TfidfVectorizer(min_df=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = {clf_name: [] for clf_name in clfs}\n",
    "\n",
    "# for train_index, test_index in skf.split(X_train, y_train):\n",
    "#     x_train_fold = []\n",
    "#     x_test_fold = []\n",
    "#     y_train_fold = []\n",
    "#     y_test_fold = []\n",
    "    \n",
    "#     for i in train_index:\n",
    "#         x_train_fold.append(X_train[i])\n",
    "#         y_train_fold.append(y_train[i])\n",
    "        \n",
    "#     for i in test_index:\n",
    "#         x_test_fold.append(X_train[i])\n",
    "#         y_test_fold.append(y_train[i])\n",
    "        \n",
    "#     x_train_fold = pipeline.fit_transform(x_train_fold, y_train_fold)\n",
    "#     x_test_fold = pipeline.transform(x_test_fold)\n",
    "    \n",
    "#     for clf_name, clf in tqdm(clfs.items()):\n",
    "#         clf.fit(x_train_fold, y_train_fold)\n",
    "#         y_pred = clf.predict(x_test_fold)\n",
    "    \n",
    "#         accuracies[clf_name].append(accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "# print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = dict(vectorizer__word__min_df=[3, 5, 7], lr__C=[0.1, 0.5, 1], lr__penalty=[\"l1\", \"l2\"])\n",
    "svm_params = dict(vectorizer__word__min_df=[3, 5, 7], svm__C=[0.01, 0.05, 0.1, 0.5])\n",
    "naive_bayes_params = dict(vectorizer__word__min_df=[3, 5, 7], naive_bayes__alpha=[0.01, 0.1, 1])\n",
    "tree_params = dict(vectorizer__word__min_df=[3, 5, 7], tree__max_depth=[None, 7, 10],\n",
    "                tree__min_samples_leaf=[1, 10])\n",
    "adaboost_params = dict(vectorizer__word__min_df=[3, 5, 7], adaboost__base_estimator=[DecisionTreeClassifier(max_depth=1),\n",
    "                        DecisionTreeClassifier(max_depth=2)])\n",
    "random_forest_params = dict(vectorizer__word__min_df=[3, 5, 7], random_forest__max_depth=[None, 10],\n",
    "                       random_forest__min_samples_leaf=[1, 5])\n",
    "extra_trees_params = dict(vectorizer__word__min_df=[3, 5, 7], extra_trees__max_depth=[None, 15],\n",
    "                       extra_trees__min_samples_leaf=[1, 15])\n",
    "lgbm_params = dict(vectorizer__word__min_df=[3, 5, 7], lgbm__min_split_gain=[0, 0.5], lgbm__colsample_bytree=[0.25, 0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"lexical\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-2bc1385e555a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m clfs = {\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     \"naive_bayes\": [naive_bayes, naive_bayes_params],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m      \u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_params\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#         \"svm\": [svm, svm_params],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#         \"tree\": [tree, tree_params],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "clfs = {\n",
    "#     \"naive_bayes\": [naive_bayes, naive_bayes_params],\n",
    "     \"lr\": [lr, lr_params],\n",
    "#         \"svm\": [svm, svm_params],\n",
    "#         \"tree\": [tree, tree_params],\n",
    "#         \"adaboost\": [adaboost, adaboost_params],\n",
    "#          \"random_forest\": [random_forest, random_forest_params],\n",
    "#          \"extra_trees\": [extra_trees, extra_trees_params],\n",
    "#         \"lgbm\": [lgbm, lgbm_params]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefs(estimator, n_top_features, filename):\n",
    "     \n",
    "    coef = estimator[-1].coef_\n",
    "    n_classes = coef.shape[0]\n",
    "    feature_names = np.array(estimator.named_steps['vectorizer'].get_feature_names())\n",
    "    print(len(feature_names))\n",
    "    \n",
    "\n",
    "    for n in range(n_classes):\n",
    "        print('class', n)\n",
    "        coefs = np.argsort(coef[n])\n",
    "        \n",
    "        if len(feature_names) > 2 * n_top_features:\n",
    "            # индексы признаков, получивших cамые большие положительные коэффициенты\n",
    "            pos_coefs = coefs[-n_top_features:]\n",
    "\n",
    "            # индексы признаков, получивших самые низкие отрицательные коэффициенты\n",
    "            neg_coefs = coefs[:n_top_features]\n",
    "\n",
    "            interesting_coefs = np.hstack([neg_coefs, pos_coefs])\n",
    "\n",
    "            plt.figure(figsize=(9, 3))\n",
    "            colors = [\"red\" if c < 0 else \"green\" for c in coef[n][interesting_coefs]]\n",
    "            plt.bar(np.arange(2 * n_top_features), coef[n][interesting_coefs], color=colors)\n",
    "            plt.xticks(np.arange(2 * n_top_features), feature_names[interesting_coefs], rotation=90, ha=\"right\")\n",
    "        else:\n",
    "            colors = [\"red\" if c < 0 else \"green\" for c in coef[n][coefs]]\n",
    "            plt.bar(np.arange(len(feature_names)), coef[n][coefs], color=colors)\n",
    "            plt.xticks(np.arange(len(feature_names)), feature_names[coefs], rotation=90, ha=\"right\")\n",
    "\n",
    "        plt.savefig(f'{filename}_class{n}.png', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(estimator, n_top_features, filename):\n",
    "    coef = estimator[-1].feature_importances_\n",
    "    feature_names = np.array(estimator.named_steps['vectorizer'].get_feature_names())\n",
    "    word_importances = pd.Series(coef, index=feature_names).sort_values(ascending=False)[:10]\n",
    "    word_importances.plot(kind='bar')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.X = X\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zipf_freqs_1', 'zipf_freqs_2', 'zipf_freqs_3', 'zipf_freqs_4',\n",
       "       'zipf_freqs_5', 'zipf_freqs_6', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2',\n",
       "       'type_token_ratio', 'abstract_nouns', 'concrete_nouns', 'lemmas'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_lexical_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_features = ['zipf_freqs_1', 'zipf_freqs_2', 'zipf_freqs_3', 'zipf_freqs_4',\n",
    "       'zipf_freqs_5', 'zipf_freqs_6', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2',\n",
    "       'type_token_ratio', 'abstract_nouns', 'concrete_nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "            [('word', vectorizer, 'lemmas'), \n",
    "            ('feature', PassthroughTransformer(), [*lexical_features])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(y_actual, y_pred):\n",
    "    acc = accuracy_score(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred, average=\"macro\")\n",
    "    print(f\"Accuracy: {acc:.4f}\\nF1 macro: {f1:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 16})\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ComplementNB object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-2d3029cff4cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_params\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtree_based\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectorizer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_trans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ComplementNB object"
     ]
    }
   ],
   "source": [
    "for clf_name, clf_params in clfs.items():\n",
    "    clf, params = clf_params\n",
    "    if clf_name in tree_based:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", column_trans), (clf_name, clf)])\n",
    "    else:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", column_trans), (\"scaler\", scaler), (clf_name, clf)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params,\n",
    "                           scoring={\"F1\": \"f1_macro\", \"Accuracy\": \"accuracy\"},\n",
    "                           refit=\"F1\", return_train_score=True, cv=skf, verbose=10) \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_f1_val = round(grid_search.best_score_, 4)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best f1 on validation: {best_f1_val}\")\n",
    "    print(\"Best parameters:\", best_params, \"\\n\")\n",
    "    estimator = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "    acc, f1 = quality(y_test, y_pred_test)\n",
    "    results[\"lexical\"][clf_name] = {\"F1 macro (validation)\": best_f1_val,\n",
    "                                    \"Best params\": best_params,\n",
    "                                    \"Accuracy (test)\": round(acc, 4),\n",
    "                                    \"F1 macro (test)\": round(f1, 4)}\n",
    "        \n",
    "    if clf_name in tree_based:\n",
    "        feature_names = visualize_feature_importances(estimator, 10, clf_name)\n",
    "    elif clf_name in with_coefs:\n",
    "        visualize_coefs(estimator, 10, clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
